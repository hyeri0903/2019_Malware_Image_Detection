import os
import configparser
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
import copy
from sklearn.model_selection import KFold
import PIL.Image

PIL.Image.MAX_IMAGE_PIXELS = 1000000000

# config parameters
config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())
config.read('config.ini')

mal_path = config.get('PATH', 'MAL_IMG_DIR')
ben_path = config.get('PATH', 'BEN_IMG_DIR')

test_mal_path = config.get('PATH', 'TEST_MAL_IMG_DIR')
test_ben_path = config.get('PATH', 'TEST_BEN_IMG_DIR')

#DATA DIR for Resnet
train_mal_path = config.get('PATH', 'TRAIN_MAL_DIR')
val_mal_path = config.get('PATH', 'VAL_MAL_DIR')

train_ben_path = config.get('PATH', 'TRAIN_BEN_DIR')
val_ben_path = config.get('PATH', 'VAL_BEN_DIR')


def make_file_list(input_path):
    fname = list()
    for path, dirs, files in os.walk(input_path):
        for file in files:
            fname.append(file)

    return fname


# sklearn Kfold 모듈을 사용해서 data를 index 기반으로 train / validation split 하는 함수
def data_split_idx(file_list, k):
    kf = KFold(k)
    train_idx = list()
    val_idx = list()
    # enumerate splits
    for train, validation in kf.split(file_list):
        # print('train: {}, test: {}'.format(train, test))
        train_idx.append(train)
        val_idx.append(validation)

    return train_idx, val_idx


# if isSplit = 1, malware data split with (train / val) folder
# else combine data from each (train/ val) data
def malware_train_val(mal_files, train_mal_idx, val_mal_idx, fold, isSplit):
    for i in range(len(train_mal_idx[fold])):
        fname = mal_files[train_mal_idx[fold][i]]
        if (isSplit):
            os.rename(os.path.join(mal_path, fname), os.path.join(train_mal_path, fname))
        else:
            os.rename(os.path.join(train_mal_path, fname), os.path.join(mal_path, fname))

    for j in range(len(val_mal_idx[fold])):
        fname = mal_files[val_mal_idx[fold][j]]
        if (isSplit):
            os.rename(os.path.join(mal_path, fname), os.path.join(val_mal_path, fname))
        else:
            os.rename(os.path.join(val_mal_path, fname), os.path.join(mal_path, fname))


# if isSplit = 1, benign data split with (train / val) folder
# else combine data from each (train/ val) data
def benign_train_val(ben_files, train_ben_idx, val_ben_idx, fold, isSplit):
    for i in range(len(train_ben_idx[fold])):
        fname = ben_files[train_ben_idx[fold][i]]
        if (isSplit):
            os.rename(os.path.join(ben_path, fname), os.path.join(train_ben_path, fname))
        else:
            os.rename(os.path.join(train_ben_path, fname), os.path.join(ben_path, fname))

    for j in range(len(val_ben_idx[fold])):
        fname = ben_files[val_ben_idx[fold][j]]
        if (isSplit):
            os.rename(os.path.join(ben_path, fname), os.path.join(val_ben_path, fname))
        else:
            os.rename(os.path.join(val_ben_path, fname), os.path.join(ben_path, fname))


#if isTest = 1, each malware train / test data move (train / val) folder
#else reset folder
def malware_train_test(isTest):
    #Test = 1이면 malware img를 train / val 폴더로 이동
    #Test = 0 이면 다시 원래 폴더로 파일 이동
    if(isTest):
        mal_files = make_file_list(mal_path)
        test_mal_files = make_file_list(test_mal_path)
        for i in range(len(mal_files)):  # Train data
            fname = mal_files[i]
            os.rename(os.path.join(mal_path, fname), os.path.join(train_mal_path, fname))
        for j in range(len(test_mal_files)):  # Test data
            fname = test_mal_files[j]
            os.rename(os.path.join(test_mal_path, fname), os.path.join(val_mal_path, fname))
    else:
        mal_files = make_file_list(train_mal_path)
        test_mal_files = make_file_list(val_mal_path)
        for i in range(len(mal_files)):  #Train data
            fname = mal_files[i]
            os.rename(os.path.join(train_mal_path, fname), os.path.join(mal_path, fname))
        for j in range(len(test_mal_files)):  # Test data
            fname = test_mal_files[j]
            os.rename(os.path.join(val_mal_path, fname), os.path.join(test_mal_path, fname))

#if isTest = 1, each benign train / test data move (train / val) folder
#else reset folder
def benign_train_test(isTest):
    #Test = 1이면 malware img를 train / test 폴더로 이동
    #Test = 0 이면 다시 원래 폴더로 파일 이동
    if(isTest):
        ben_files = make_file_list(ben_path)
        test_ben_files = make_file_list(test_ben_path)
        for i in range(len(ben_files)):
            fname = ben_files[i]
            os.rename(os.path.join(ben_path, fname), os.path.join(train_ben_path, fname))
        for j in range(len(test_ben_files)):
            fname = test_ben_files[j]
            os.rename(os.path.join(test_ben_path, fname), os.path.join(val_ben_path, fname))
    else:
        ben_files = make_file_list(train_ben_path)
        test_ben_files = make_file_list(val_ben_path)
        for i in range(len(ben_files)):
            fname = ben_files[i]
            os.rename(os.path.join(train_ben_path, fname), os.path.join(ben_path, fname))
        for j in range(len(test_ben_files)):
            fname = test_ben_files[j]
            os.rename(os.path.join(val_ben_path, fname), os.path.join(test_ben_path, fname))

# Model을 교차검증하는 함수
def cv_model(mal_files, ben_files, criterion, num_epochs, k):
    # train 과 validation 할 데이터를 split 하고index 를 저장
    train_mal_idx, val_mal_idx = data_split_idx(mal_files, k)
    train_ben_idx, val_ben_idx = data_split_idx(ben_files, k)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    best_val_acc_list = []  # k개 만큼 best val_acc 저장
    startTime = time.time()
    # k-fold의 k만큼 train - val 반복
    for i in range(k):
        since = time.time()
        #### Fine Tuning ####
        # Pre-trained Resnet50
        print("Load Pre-trained Resnet50\n")
        model = models.resnet50(pretrained=True)
        # Freeze model
        for param in model.parameters():
            param.requires_grad = False

        num_ftrs = model.fc.in_features  # num inputs
        model.fc = nn.Linear(num_ftrs, 2)  # FC 층을 재설정하는 부분
        model = model.to(device)

        # Observe that all parameters are being optimized
        lr = float(config.get('CLASSIFIER', 'LEARNING_RATE'))
        optimizer = optim.Adam(model.parameters(), lr)

        ##############################  Train / Val Split  ##################################
        print("{}/{} split data".format(i + 1, k))
        print("\n")
        # 각각의 malware 와 benign data를 train, val 폴더에 이동
        malware_train_val(mal_files, train_mal_idx, val_mal_idx, fold = i, isSplit = 1)
        benign_train_val(ben_files, train_ben_idx, val_ben_idx, fold = i, isSplit = 1)

        ############################# dataloader ###############################
        # load data from folder each k fold cv
        data_transforms = {
            'train': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
        }

        data_dir = config.get('PATH', 'DATA_DIR')
        image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}

        # dataloader : multiple sample load
        batch_size = int(config.get('CLASSIFIER', 'BATCH_SIZE'))
        num_workers = int(config.get('CLASSIFIER', 'NUM_WORKERS'))
        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in ['train', 'val']}

        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
        class_names = image_datasets['train'].classes

        best_val_acc = 0.0
        ################################ Train ##################################
        # epoch 만큼 data 학습 & 검증
        for epoch in range(num_epochs):
            print('Epoch {}/{}'.format(epoch + 1, num_epochs))
            print('-' * 10)

            for phase in ['train','val']:
                print("phase:", phase)
                if phase == 'train':
                    model.train()  # Set model to training mode
                else:
                    model.eval() #Set model to evaluate mode
                    
                running_loss = 0.0
                running_corrects = 0
    
                # Iterate over data.
                for inputs, labels in dataloaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)
    
                    # zero the parameter gradients
                    optimizer.zero_grad()
    
                    # forward
                    # track history if only in train
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)  # 한 행에서 가장 큰 값 반환, 큰 값의 index값
                        loss = criterion(outputs, labels)
    
                        # backward + optimize only if in training phase
                        if phase == 'train':
                            loss.backward()
                            optimizer.step()
    
                    # statistics , loss.item() : loss의 스칼라 값
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)

                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]

                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

                ############################# Best val accuracy ################################
                if phase == 'val' and epoch_acc > best_val_acc:
                    best_val_acc = epoch_acc

        print("Finish Total Epoch Train")
        best_val_acc_list.append(best_val_acc)
        print()

        time_elapsed = time.time() - since
        print('Train & Val complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        print('Best val Acc: {:4f}'.format(best_val_acc))

        print("\n")
        # epoch 끝나면 train/val 폴더에 있던 데이터를 모두 원래 폴더로 다시 이동
        malware_train_val(mal_files, train_mal_idx, val_mal_idx, fold=i, isSplit=0)
        benign_train_val(ben_files, train_ben_idx, val_ben_idx, fold=i, isSplit=0)
        print("{}/{} combine data".format(i + 1, k))
        print("\n")

    print("Avg k-fold validation accuracy : {}".format(sum(best_val_acc_list) / k))
    endTime = time.time() - startTime
    print('K-fold complete in {:.0f}m {:.0f}s'.format(endTime// 60,  endTime% 60))


def test_model(criterion, num_epochs):
    #move each train /test data to (train / val) folder
    print("malware and benign data move to train / val folder in Test mode")
    malware_train_test(isTest=1)
    benign_train_test(isTest=1)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    since = time.time()

    print("Load Pre-trained Resnet50\n")
    model = models.resnet50(pretrained=True)
    #Freeze model
    for param in model.parameters():
        param.requires_grad = False

    num_ftrs = model.fc.in_features  # num inputs
    model.fc = nn.Linear(num_ftrs, 2)  # FC 층을 재설정하는 부분
    model = model.to(device)

    # Observe that all parameters are being optimized
    lr = float(config.get('CLASSIFIER', 'LEARNING_RATE'))
    optimizer = optim.Adam(model.parameters(), lr)

    ############################# dataloader ###############################
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    data_dir = config.get('PATH', 'DATA_DIR')
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}

    # dataloader : multiple sample load
    batch_size = int(config.get('CLASSIFIER', 'BATCH_SIZE'))
    num_workers = int(config.get('CLASSIFIER', 'NUM_WORKERS'))
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in ['train', 'val']}

    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
    class_names = image_datasets['train'].classes

    best_val_acc = 0.0
    ################################ Train ##################################
    # epoch 만큼 train data 학습
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch + 1, num_epochs))
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)  # 한 행에서 가장 큰 값 반환, 큰 값의 index값
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics , loss.item() : loss의 스칼라 값
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

        # deep copy the model
        if phase == 'val' and epoch_acc > best_val_acc:
            best_val_acc = epoch_acc
            best_model_wts = copy.deepcopy(model.state_dict())

    print("Finish Total Epoch Train & Val")
    print()

    time_elapsed = time.time() - since
    print('Train & Val complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_val_acc))

    print("\nStart val using Best Model")
    # load best model weights
    model.load_state_dict(best_model_wts)
    phase = 'val'
    model.eval()

    running_loss = 0.0
    running_corrects = 0.0

    # Iterate over data.
    for inputs, labels in dataloaders[phase]:
        inputs = inputs.to(device)
        labels = labels.to(device)
        # zero the parameter gradients
        optimizer.zero_grad()
        # forward
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)  # 한 행에서 가장 큰 값 반환, 큰 값의 index값
        loss = criterion(outputs, labels)

        # statistics , loss.item() : loss의 스칼라 값
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

    val_loss = running_loss / dataset_sizes['val']
    val_acc = running_corrects.double() / dataset_sizes['val']

    print('Final val Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))

    print("\n")
    # Test가 끝나면 train/val 폴더에 있던 데이터를 모두 원래 폴더로 다시 이동
    malware_train_test(isTest = 0)
    benign_train_test(isTest = 0)
    print("malware and benign data reset folder")


if __name__ == "__main__":
    # 파일의 이름을 리스트에 저장
    mal_files = make_file_list(mal_path)
    ben_files = make_file_list(ben_path)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    num_epoch = int(config.get('CLASSIFIER', 'EPOCH'))
    k = int(config.get('CLASSIFIER', 'K_FOLD_VALUE'))

    cv_model(mal_files, ben_files, criterion, num_epoch, k)
    #test_model(criterion, num_epoch)

    print("\n ====== Finish ====== \n")

import os
import configparser
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
import copy
from sklearn.model_selection import KFold
import PIL.Image

import torch.nn.functional as F
from torch.autograd import Variable

PIL.Image.MAX_IMAGE_PIXELS = 1000000000
'''
cnn : 3 > 6 > 12 > 24 > 48
fc : (14*14*48)9048 -> 128 -> 16 -> 2
'''
# config parameters
config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())
config.read('config.ini')

mal_path = config.get('PATH', 'MAL_IMG_DIR')
ben_path = config.get('PATH', 'BEN_IMG_DIR')

test_mal_path = config.get('PATH', 'TEST_MAL_IMG_DIR')
test_ben_path = config.get('PATH', 'TEST_BEN_IMG_DIR')

#DATA DIR for Resnet
train_mal_path = config.get('PATH', 'TRAIN_MAL_DIR')
val_mal_path = config.get('PATH', 'VAL_MAL_DIR')

train_ben_path = config.get('PATH', 'TRAIN_BEN_DIR')
val_ben_path = config.get('PATH', 'VAL_BEN_DIR')



def make_file_list(input_path):
    fname = list()
    for path, dirs, files in os.walk(input_path):
        for file in files:
            fname.append(file)

    return fname

# sklearn Kfold 모듈을 사용해서 data를 index 기반으로 train / validation split 하는 함수
def data_split_idx(file_list, k):
    kf = KFold(k)
    train_idx = list()
    val_idx = list()
    # enumerate splits
    for train, validation in kf.split(file_list):
        # print('train: {}, test: {}'.format(train, test))
        train_idx.append(train)
        val_idx.append(validation)

    return train_idx, val_idx

def malware_train_val(mal_files, train_mal_idx, val_mal_idx, fold, isSplit):
    for i in range(len(train_mal_idx[fold])):
        fname = mal_files[train_mal_idx[fold][i]]
        if (isSplit):
            os.rename(os.path.join(mal_path, fname), os.path.join(train_mal_path, fname))
        else:
            os.rename(os.path.join(train_mal_path, fname), os.path.join(mal_path, fname))

    for j in range(len(val_mal_idx[fold])):
        fname = mal_files[val_mal_idx[fold][j]]
        if (isSplit):
            os.rename(os.path.join(mal_path, fname), os.path.join(val_mal_path, fname))
        else:
            os.rename(os.path.join(val_mal_path, fname), os.path.join(mal_path, fname))


# if isSplit = 1, benign data split with (train / val) folder
# else combine data from each (train/ val) data
def benign_train_val(ben_files, train_ben_idx, val_ben_idx, fold, isSplit):
    for i in range(len(train_ben_idx[fold])):
        fname = ben_files[train_ben_idx[fold][i]]
        if (isSplit):
            os.rename(os.path.join(ben_path, fname), os.path.join(train_ben_path, fname))
        else:
            os.rename(os.path.join(train_ben_path, fname), os.path.join(ben_path, fname))

    for j in range(len(val_ben_idx[fold])):
        fname = ben_files[val_ben_idx[fold][j]]
        if (isSplit):
            os.rename(os.path.join(ben_path, fname), os.path.join(val_ben_path, fname))
        else:
            os.rename(os.path.join(val_ben_path, fname), os.path.join(ben_path, fname))


def malware_train_test(isTest):
    #Test = 1이면 malware img를 train / test 폴더로 이동
    #Test = 0 이면 다시 원래 폴더로 파일 이동
    if(isTest):
        mal_files = make_file_list(mal_path)
        test_mal_files = make_file_list(test_mal_path)
        for i in range(len(mal_files)):
            fname = mal_files[i]
            os.rename(os.path.join(mal_path, fname), os.path.join(train_mal_path, fname))
        for j in range(len(test_mal_files)):
            fname = test_mal_files[j]
            os.rename(os.path.join(test_mal_path, fname), os.path.join(val_mal_path, fname))
    else:
        mal_files = make_file_list(train_mal_path)
        test_mal_files = make_file_list(val_mal_path)
        for i in range(len(mal_files)):
            fname = mal_files[i]
            os.rename(os.path.join(train_mal_path, fname), os.path.join(mal_path, fname))
        for j in range(len(test_mal_files)):
            fname = test_mal_files[j]
            os.rename(os.path.join(val_mal_path, fname), os.path.join(test_mal_path, fname))

def benign_train_test(isTest):
    #Test = 1이면 malware img를 train / test 폴더로 이동
    #Test = 0 이면 다시 원래 폴더로 파일 이동
    if(isTest):
        ben_files = make_file_list(ben_path)
        test_ben_files = make_file_list(test_ben_path)
        for i in range(len(ben_files)):
            fname = ben_files[i]
            os.rename(os.path.join(ben_path, fname), os.path.join(train_ben_path, fname))
        for j in range(len(test_ben_files)):
            fname = test_ben_files[j]
            os.rename(os.path.join(test_ben_path, fname), os.path.join(val_ben_path, fname))
    else:
        ben_files = make_file_list(train_ben_path)
        test_ben_files = make_file_list(val_ben_path)
        for i in range(len(ben_files)):
            fname = ben_files[i]
            os.rename(os.path.join(train_ben_path, fname), os.path.join(ben_path, fname))
        for j in range(len(test_ben_files)):
            fname = test_ben_files[j]
            os.rename(os.path.join(val_ben_path, fname), os.path.join(test_ben_path, fname))

#CNN 정의
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        #input is 224x224
        #padding=2 for same padding
        output_size = 2 #binary classification
        # convolutional layer
        # 입력데이터 채널 수, 출력 채널 수, 커널사이즈, 패딩=2 아니면 1
        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(6, 12, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(12, 24, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(24, 48, kernel_size=3, padding=1)
        # feature map size is 14*14 by pooling
        self.fc1 = nn.Linear(14*14*48, 128)
        self.fc2 = nn.Linear(128, 16)
        self.fc3 = nn.Linear(16, output_size)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = F.max_pool2d(F.relu(self.conv3(x)), 2)
        x = F.max_pool2d(F.relu(self.conv4(x)), 2)
        dim = 1
        #make Linear
        for d in x.size()[1:]: #14*14*48
            dim = dim * d
        x = x.view(-1, dim) #reshape Variable
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def cv_model(mal_files, ben_files, criterion, num_epochs, k):
    # train 과 validation 할 데이터를 split 하고index 를 저장
    train_mal_idx, val_mal_idx = data_split_idx(mal_files, k)
    train_ben_idx, val_ben_idx = data_split_idx(ben_files, k)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    since = time.time()
    best_val_acc_list = []  # k개 만큼 best acc 저장
    #total_val_acc = 0.0
    #best_val_acc = 0.0
    #best_model_wts = copy.deepcopy(model.state_dict())  # model의 parameter 값들을 key-val 형태로 저장

    # k-fold의 k만큼 train - val 반복
    for i in range(k):
        print("Load CNN Model\n")
        model = Net()
        model = model.to(device)

        # Observe that all parameters are being optimized
        lr = 0.0001
        optimizer = optim.Adam(model.parameters(), lr)

        ##############################  Train / Val Split  ##################################
        print("{}/{} split data".format(i + 1, k))
        print("\n")
        # 각각의 malware 와 benign data를 train, val 폴더에 이동
        benign_train_val(ben_files, train_ben_idx, val_ben_idx, fold = i, isSplit = 1)
        malware_train_val(mal_files, train_mal_idx, val_mal_idx, fold = i, isSplit = 1)

        ############################# dataloader ###############################
        # k 번째 fold 때마다 dataloader를 실행시켜주어야 함
        data_transforms = {
            'train': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
        }

        data_dir =  r'/home/data_cv/'
        image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}

        # dataloader : multiple sample load
        batch_size = 32
        num_workers = 0
        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}

        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
        class_names = image_datasets['train'].classes

        best_val_acc = 0.0
        ################################ Train ##################################
        # epoch 만큼 train data 학습
        for epoch in range(num_epochs):
            print('Epoch {}/{}'.format(epoch + 1, num_epochs))
            print('-' * 10)

            for phase in ['train', 'val']:
                if (phase == 'train'):
                    model.train()  # Set model to training mode
                else:
                    model.eval()

                running_loss = 0.0
                running_corrects = 0

                # Iterate over data.
                for inputs, labels in tqdm(dataloaders[phase]):
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    # zero the parameter gradients
                    optimizer.zero_grad()

                    # forward
                    # track history if only in train
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)  # 한 행에서 가장 큰 값 반환, 큰 값의 index값
                        loss = criterion(outputs, labels)

                        # backward + optimize only if in training phase
                        if phase == 'train':
                            loss.backward()
                            optimizer.step()

                    # statistics , loss.item() : loss의 스칼라 값
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)

                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]

                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

                ############################# Best val accuracy ################################
                if phase == 'val' and epoch_acc > best_val_acc:
                    best_val_acc = epoch_acc

        print("Finish Total Epoch Train")
        best_val_acc_list.append(best_val_acc)
        print()

        time_elapsed = time.time() - since
        print('1-fold Train & Val complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
        print('Best val Acc: {:4f}'.format(best_val_acc))

        print("\n")
        # epoch 끝나면 train/val 폴더에 있던 데이터를 모두 원래 폴더로 다시 이동
        malware_train_val(mal_files, train_mal_idx, val_mal_idx, fold=i, isSplit=0)
        benign_train_val(ben_files, train_ben_idx, val_ben_idx, fold=i, isSplit=0)
        print("{}/{} combine data".format(i + 1, k))
        print("\n")

    print("Avg validation accuracy : {}".format(sum(best_val_acc_list) / k))
    end_cv = time.time() - since
    print("Finish CV model in {:.0f}m {:.0f}s'.format(end_cv // 60, end_cv % 60))


def test_model(criterion, num_epochs):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    since = time.time()

    print("Load CNN Model\n")
    model = Net()
    #num_ftrs = model.fc.in_features  # num inputs
    #model.fc = nn.Linear(num_ftrs, 2)  # FC 층을 재설정하는 부분
    model = model.to(device)

    # Observe that all parameters are being optimized
    lr = float(config.get('CLASSIFIER', 'LEARNING_RATE'))
    optimizer = optim.Adam(model.parameters(), lr)

    ############################# dataloader ###############################
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    data_dir = config.get('PATH', 'DATA_DIR')
    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}

    # dataloader : multiple sample load
    batch_size = int(config.get('CLASSIFIER', 'BATCH_SIZE'))
    num_workers = int(config.get('CLASSIFIER', 'NUM_WORKERS'))
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=num_workers) for x in ['train', 'val']}

    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
    class_names = image_datasets['train'].classes

    best_val_acc = 0.0
    ################################ Train ##################################
    # epoch 만큼 train data 학습
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch + 1, num_epochs))
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)
                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)  # 한 행에서 가장 큰 값 반환, 큰 값의 index값
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics , loss.item() : loss의 스칼라 값
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

        # deep copy the model
        if phase == 'val' and epoch_acc > best_val_acc:
            best_val_acc = epoch_acc
            best_model_wts = copy.deepcopy(model.state_dict())

    print("Finish Total Epoch Train & Val")
    print()

    time_elapsed = time.time() - since
    print('Train & Val complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_val_acc))
    # ---------------------------------------------------
    print("\n*** Start val using Best Model ***")
    # load best model weights
    model.load_state_dict(best_model_wts)
    phase = 'val'
    model.eval()

    running_loss = 0.0
    running_corrects = 0.0

    # Iterate over data.
    for inputs, labels in dataloaders[phase]:
        inputs = inputs.to(device)
        labels = labels.to(device)
        # zero the parameter gradients
        optimizer.zero_grad()
        # forward
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)  # 한 행에서 가장 큰 값 반환, 큰 값의 index값
        loss = criterion(outputs, labels)

        # statistics , loss.item() : loss의 스칼라 값
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

    val_loss = running_loss / dataset_sizes['val']
    val_acc = running_corrects.double() / dataset_sizes['val']

    print('Final val Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc))


if __name__ == "__main__":
    # 파일의 이름을 리스트에 저장
    #mal_files = make_file_list(mal_path)
    #ben_files = make_file_list(ben_path)

    # Loss function
    criterion = nn.CrossEntropyLoss()
    num_epoch = int(config.get('CLASSIFIER', 'EPOCH'))
    #cv_model(mal_files, ben_files, criterion, num_epoch, k)

    # move each train /test data to (train / val) folder
    print("malware and benign data move to train / val folder in Test mode")
    #malware_train_test(isTest=1)
    #benign_train_test(isTest=1)

    test_model(criterion, num_epoch)
    # Test가 끝나면 train/val 폴더에 있던 데이터를 모두 원래 폴더로 다시 이동
    # malware_train_test(isTest=0)
    # benign_train_test(isTest=0)
    # print("malware and benign data reset folder")

